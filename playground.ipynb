{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "# len of data\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size:  65\n"
     ]
    }
   ],
   "source": [
    "# unique chars\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "unique_chars = ''.join(chars)   \n",
    "print(unique_chars)\n",
    "print(\"Vocab size: \", vocab_size)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 57, 47, 56]\n",
      "hello sir\n"
     ]
    }
   ],
   "source": [
    "# create a mapping of unique chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# encode and decode functions\n",
    "encode = lambda s : [char_to_int[i] for i in s]\n",
    "decode = lambda s : ''.join([int_to_char[i] for i in s])\n",
    "\n",
    "ex = \"hello sir\"\n",
    "print(encode(ex))\n",
    "print(decode(encode(ex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# encode data \n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "training_perc = 0.8\n",
    "n = int(len(data) * training_perc)\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[58, 63,  8,  0,  0, 19, 24, 27],\n",
      "        [39, 59, 45, 46, 58,  1, 46, 43],\n",
      "        [49, 43, 57,  1, 53, 50, 42,  1],\n",
      "        [52, 41, 47, 43, 52, 58,  1, 56]])\n",
      "---------------------------\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[63,  8,  0,  0, 19, 24, 27, 33],\n",
      "        [59, 45, 46, 58,  1, 46, 43,  1],\n",
      "        [43, 57,  1, 53, 50, 42,  1, 46],\n",
      "        [41, 47, 43, 52, 58,  1, 56, 47]])\n"
     ]
    }
   ],
   "source": [
    "# for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4 # Independent sequences will we process in parallel (B)\n",
    "block_size = 8 # Maximum context length for predictions (T)\n",
    "# vocab size (C)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb) # input to the transformer\n",
    "print('---------------------------')\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0493, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([32, 65])\n",
      "tensor([[ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064],\n",
      "        [ 1.1407,  0.8935, -2.4000,  ...,  0.3227,  1.5431, -1.0392],\n",
      "        ...,\n",
      "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # [batch_size, block_size, vocab_size]\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # define the loss function\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(idx)\n",
    "            # focus on the last token\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # append the new token to the context\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "        return idx\n",
    "            \n",
    "\n",
    "# create the model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(loss)\n",
    "print(logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ffound ds de r s; gq--k!\n",
      "ICAR:CAhar hurtDUxfonsopouQUCl:&PHai\n",
      "Agr tylo o'\n",
      "\n",
      "Nof3ullicdfoThiscolly s3Q! ty iscad ort,SBEy,\n",
      "Alathal thid?-t Rofu,\n",
      "Aclldato ld se ne, hathJXXMAm; lr ay menienearsenx'd themy Bemewiada and lit wothorou t\n",
      "R\n",
      "Ad ther cowaf stot?3Wh$'s\n",
      "DYom\n",
      "\n",
      "Yo'stTMqu ?she:\n",
      "ARo G, ickiceirt izr EHUSusoS:\n",
      "Ll\n",
      "\n",
      "Mat gKI'sthe, hok.\n",
      "Fofelin th,\n",
      "\n",
      "'lolly!\n",
      "To, q; s SDWhagapxErers agq-lullulea;\n",
      "Ame w ANYRI: totof Tr n,\n",
      "\n",
      "Y:\n",
      "xccende gssthoiun norode,\n",
      "A se iJGhe: rre sammoun \n",
      "TwNXmsHot'Thes smeth$R:\n",
      "PG\n"
     ]
    }
   ],
   "source": [
    "# get the logits and loss without training\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "predictions = model.generate(idx=idx, max_new_tokens=500)[0].tolist()\n",
    "decoded_predictions = decode(predictions)\n",
    "print(decoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4823920726776123\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "max_iterations = 5000 \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(max_iterations): # increase number of steps for good results...\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BR: ct\n",
      "Ywit harfoul'st, ar izent t ct.\n",
      "Fo, sther:\n",
      "I d tre th,-she.\n",
      "Wowstothedind rtee t the t,\n",
      "STo way!\n",
      "Thowest e tistloue ded ndean-bros g qpl mout fok yolaime do myo asto,\n",
      "Mok h ay t nch sle fionhoured whaneables mye.\n",
      "For f beng tho; ar!TCald? min, wherure;\n",
      "Fing tyoucoracumad s?\n",
      "\n",
      "Tord, g I:\n",
      "Whireat pr f Yhy? menere hurer cr il f aloulatspribr,\n",
      "AG o otr thall oull\n",
      "NRartheat.\n",
      "We\n",
      "\n",
      "GAseraizL: ag, t wild he boro g s bl?\n",
      "DWhapato' h bls whoommeulye Werngssamyou\n",
      "Shar, hed wingoleshet heked\n",
      "GUExf te, \n"
     ]
    }
   ],
   "source": [
    "# get the logits after training\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "predictions = model.generate(idx=idx, max_new_tokens=500)[0].tolist()\n",
    "decoded_predictions = decode(predictions)\n",
    "print(decoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention - one head\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# compatibility matrix\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# mask the upper triangle of the matrix to prevent the model from looking into the future\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "# apply softmax to get the weights\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.0\n",
    "\n",
    "n_embd = 64\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   \n",
    "        q = self.query(x) \n",
    "        # compute attention scores (\"affinities\") - scaled dot-product\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
    "        wei = F.softmax(wei, dim=-1) \n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) \n",
    "        out = wei @ v \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_embedding = self.token_embedding_table(idx)  # [batch_size, block_size, vocab_size]\n",
    "        position_embedding = self.position_embedding_table(torch.arange(T, device=device)) # [block_size, vocab_size]\n",
    "        x = token_embedding + position_embedding # [batch_size, block_size, vocab_size]\n",
    "        x = self.sa_head(x) # apply self-attention\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # define the loss function\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the context to the block size\n",
    "            idx_crp = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(idx_crp)\n",
    "            # focus on the last token\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # append the new token to the context\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007553 M parameters\n",
      "step 0: train loss 4.2701, val loss 4.2669\n",
      "step 500: train loss 2.6862, val loss 2.6973\n",
      "step 1000: train loss 2.5271, val loss 2.5178\n",
      "step 1500: train loss 2.4727, val loss 2.4988\n",
      "step 2000: train loss 2.4321, val loss 2.4635\n",
      "step 2500: train loss 2.4135, val loss 2.4415\n",
      "step 3000: train loss 2.4034, val loss 2.4440\n",
      "step 3500: train loss 2.4006, val loss 2.4263\n",
      "step 4000: train loss 2.3902, val loss 2.4207\n",
      "step 4500: train loss 2.3826, val loss 2.4186\n",
      "step 4999: train loss 2.3882, val loss 2.4251\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "learning_rate = 1e-3\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "n_embd = 32\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CH:\n",
      "EL: ty coonoor lpre houn; thell ar sarde ithies ghe you cokis may. BRANBupethou hanif cprdank ory lerquinout oy and be P:\n",
      "ME:\n",
      "A wofourns.\n",
      "\n",
      "Ano wore, Sd, cave, were ucey my w:? orma, balal citol oment.\n",
      "\n",
      "Wimy hen re ay whe.\n",
      "Andiunge ht iromurd hid'dy tirthe pee;\n",
      "O mte cete'sn pabrls is tuly hay.\n",
      "S.\n",
      "PBug cis. Clavet:\n",
      "Mem:\n",
      "On di till de ilin,\n",
      "am msthat, groullindgu weysonlatel BAMous thind; or:\n",
      "Who? to rust thivey; ond het bemathe, tren hell hot winede, priend to youse lt he I na swind,\n",
      "O agreea\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Running multiple heads in parallel\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # concatenate the outputs at the channel dimension\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # apply a linear transformation\n",
    "        out = self.projection(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # 4 heads of 8 dimensions each\n",
    "        self.sa_heads = MultiHeadAttention(num_heads=4, head_size=n_embd//4)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_embedding = self.token_embedding_table(idx)  # [batch_size, block_size, vocab_size]\n",
    "        position_embedding = self.position_embedding_table(torch.arange(T, device=device)) # [block_size, vocab_size]\n",
    "        x = token_embedding + position_embedding # [batch_size, block_size, vocab_size]\n",
    "        x = self.sa_heads(x) # apply self-attention\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # define the loss function\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the context to the block size\n",
    "            idx_crp = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(idx_crp)\n",
    "            # focus on the last token\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # append the new token to the context\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007553 M parameters\n",
      "step 0: train loss 4.1932, val loss 4.1909\n",
      "step 500: train loss 2.6328, val loss 2.6509\n",
      "step 1000: train loss 2.4840, val loss 2.4913\n",
      "step 1500: train loss 2.4067, val loss 2.4392\n",
      "step 2000: train loss 2.3691, val loss 2.3991\n",
      "step 2500: train loss 2.3394, val loss 2.3795\n",
      "step 3000: train loss 2.3081, val loss 2.3488\n",
      "step 3500: train loss 2.2792, val loss 2.3404\n",
      "step 4000: train loss 2.2701, val loss 2.3156\n",
      "step 4500: train loss 2.2660, val loss 2.3046\n",
      "step 4999: train loss 2.2399, val loss 2.2960\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wor ko thou iresbereachtius malg avenceve, his tho'd andchad yor havethes ave tis you, ingnot pou fel,\n",
      "Boce epoo whame, fat. sand In:\n",
      "To pcour she fugof ther ingard Ebe Sar me he of hore in othe sad come cearl Lo:\n",
      "Vron now cenes liks ipe; fortin you thiem hin guok:\n",
      "Wheencento for of hen wens annd Soid\n",
      "Agh of vear yome the thin ur etad hince, wake\n",
      "Thak wite, my me,\n",
      "Of War the herorelwbe land caisbe homend't igh rot for thatll\n",
      "YLhauy, if:\n",
      "Sellomere sumbodbokee,\n",
      "O MAn wlesle tho, worsh? sar I thin \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple feed-forward module\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            # project back to the original dimension\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # 4 heads of 8 dimensions each\n",
    "        self.sa_heads = MultiHeadAttention(num_heads=4, head_size=n_embd//4)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_embedding = self.token_embedding_table(idx)  # [batch_size, block_size, vocab_size]\n",
    "        position_embedding = self.position_embedding_table(torch.arange(T, device=device)) # [block_size, vocab_size]\n",
    "        x = token_embedding + position_embedding # [batch_size, block_size, vocab_size]\n",
    "        x = self.sa_heads(x) # apply self-attention\n",
    "        x = self.ffwd(x) # apply the feed-forward network\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # define the loss function\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the context to the block size\n",
    "            idx_crp = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(idx_crp)\n",
    "            # focus on the last token\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # append the new token to the context\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008609 M parameters\n",
      "step 0: train loss 4.1665, val loss 4.1694\n",
      "step 500: train loss 2.5867, val loss 2.6039\n",
      "step 1000: train loss 2.4401, val loss 2.4661\n",
      "step 1500: train loss 2.3858, val loss 2.4031\n",
      "step 2000: train loss 2.3401, val loss 2.3611\n",
      "step 2500: train loss 2.2965, val loss 2.3276\n",
      "step 3000: train loss 2.2728, val loss 2.3140\n",
      "step 3500: train loss 2.2473, val loss 2.2834\n",
      "step 4000: train loss 2.2443, val loss 2.2706\n",
      "step 4500: train loss 2.2252, val loss 2.2616\n",
      "step 4999: train loss 2.2167, val loss 2.2638\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Colly setith bads, des waray wannd walded,- hat, artut ischee wieth reat ten, you; to tersomis chat wan, his?\n",
      "Wadby to And the thet?\n",
      "\n",
      "FUSWian phomy, there! eet it that the lilrave Ec\n",
      "Cou Riftteds my sive thave, thou wereers; I yave toowive, be, warke wers dooves with thauld wandy:\n",
      "Wavene.\n",
      "\n",
      "COROK:\n",
      "Thead no to I be pofory: pradive ren!\n",
      "LLIZINGARD:\n",
      "IWanCyWhat, deis:\n",
      "And consike, my you bromcut\n",
      "He;\n",
      "Whaten.\n",
      "Tenow tes the fall'dse nowsoum irjoimblave sient tear nall nocest weand cer.\n",
      "And wame hy the; \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.attn = MultiHeadAttention(num_heads=n_head, head_size=head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.1\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_embedding = self.token_embedding_table(idx)  # [batch_size, block_size, vocab_size]\n",
    "        position_embedding = self.position_embedding_table(torch.arange(T, device=device)) # [block_size, vocab_size]\n",
    "        x = token_embedding + position_embedding # [batch_size, block_size, vocab_size]\n",
    "        x = self.blocks(x) # apply the transformer blocks\n",
    "        x = self.ln_f(x) # apply layer normalization\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # define the loss function\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the context to the block size\n",
    "            idx_crp = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(idx_crp)\n",
    "            # focus on the last token\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # append the new token to the context\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.309313 M parameters\n",
      "step 0: train loss 4.3498, val loss 4.3552\n",
      "step 100: train loss 2.6389, val loss 2.6522\n",
      "step 200: train loss 2.5043, val loss 2.5301\n",
      "step 300: train loss 2.4258, val loss 2.4683\n",
      "step 400: train loss 2.3703, val loss 2.4098\n",
      "step 500: train loss 2.3094, val loss 2.3449\n",
      "step 600: train loss 2.2753, val loss 2.3009\n",
      "step 700: train loss 2.2200, val loss 2.2537\n",
      "step 800: train loss 2.1823, val loss 2.2343\n",
      "step 900: train loss 2.1430, val loss 2.2001\n",
      "step 1000: train loss 2.1127, val loss 2.1599\n",
      "step 1100: train loss 2.0744, val loss 2.1390\n",
      "step 1200: train loss 2.0503, val loss 2.1148\n",
      "step 1300: train loss 2.0197, val loss 2.0928\n",
      "step 1400: train loss 2.0135, val loss 2.0828\n",
      "step 1500: train loss 1.9821, val loss 2.0607\n",
      "step 1600: train loss 1.9531, val loss 2.0446\n",
      "step 1700: train loss 1.9500, val loss 2.0292\n",
      "step 1800: train loss 1.9278, val loss 2.0260\n",
      "step 1900: train loss 1.9137, val loss 2.0178\n",
      "step 2000: train loss 1.8964, val loss 2.0021\n",
      "step 2100: train loss 1.8824, val loss 1.9916\n",
      "step 2200: train loss 1.8707, val loss 1.9834\n",
      "step 2300: train loss 1.8509, val loss 1.9684\n",
      "step 2400: train loss 1.8464, val loss 1.9612\n",
      "step 2500: train loss 1.8207, val loss 1.9652\n",
      "step 2600: train loss 1.8171, val loss 1.9514\n",
      "step 2700: train loss 1.8037, val loss 1.9495\n",
      "step 2800: train loss 1.8143, val loss 1.9467\n",
      "step 2900: train loss 1.8089, val loss 1.9398\n",
      "step 3000: train loss 1.7821, val loss 1.9336\n",
      "step 3100: train loss 1.7846, val loss 1.9281\n",
      "step 3200: train loss 1.7761, val loss 1.9234\n",
      "step 3300: train loss 1.7583, val loss 1.9293\n",
      "step 3400: train loss 1.7606, val loss 1.9197\n",
      "step 3500: train loss 1.7538, val loss 1.9246\n",
      "step 3600: train loss 1.7362, val loss 1.9222\n",
      "step 3700: train loss 1.7361, val loss 1.9235\n",
      "step 3800: train loss 1.7290, val loss 1.9079\n",
      "step 3900: train loss 1.7268, val loss 1.9182\n",
      "step 4000: train loss 1.7196, val loss 1.8970\n",
      "step 4100: train loss 1.7245, val loss 1.9065\n",
      "step 4200: train loss 1.7128, val loss 1.9048\n",
      "step 4300: train loss 1.7008, val loss 1.8916\n",
      "step 4400: train loss 1.7012, val loss 1.8911\n",
      "step 4500: train loss 1.7032, val loss 1.8992\n",
      "step 4600: train loss 1.6928, val loss 1.8624\n",
      "step 4700: train loss 1.6781, val loss 1.8805\n",
      "step 4800: train loss 1.6955, val loss 1.8792\n",
      "step 4900: train loss 1.6759, val loss 1.8823\n",
      "step 4999: train loss 1.6777, val loss 1.8508\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A vodesed farlouker fromer: prackener cond yed lidst oldry himst lain to shall feam restartyer\n",
      "un you the beak\n",
      "laister surefer with call\n",
      "Onemplans him dot king frome:\n",
      "Afrewreome's pith I now.\n",
      "The hencioused pross with beant will beind mely.\n",
      "\n",
      "JULF CHITHENENE:\n",
      "He coldy it gracter'y pread, sunber the have so not remer, she their rest Tyour but flewty-that dooke have of it spoon nurrey bray deaster,\n",
      "Is may show seacing thildly, I to day'n thou shaln\n",
      "Stad chefout tilesedgs: runjusbit I his Ay, it wee\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
